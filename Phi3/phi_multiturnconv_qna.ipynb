{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":319600,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":269672,"modelId":290663}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:04:08.019155Z","iopub.execute_input":"2025-03-27T12:04:08.019396Z","iopub.status.idle":"2025-03-27T12:04:08.947066Z","shell.execute_reply.started":"2025-03-27T12:04:08.019373Z","shell.execute_reply":"2025-03-27T12:04:08.946369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install bitsandbytes transformers datasets accelerate peft torch torchvision torchaudio datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T16:59:12.347143Z","iopub.execute_input":"2025-04-03T16:59:12.347443Z","iopub.status.idle":"2025-04-03T16:59:20.473936Z","shell.execute_reply.started":"2025-04-03T16:59:12.347420Z","shell.execute_reply":"2025-04-03T16:59:20.473052Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"https://github.com/lisaalaz/satbot/blob/master/model/EmpatheticPersonas.csv\nlast step training on empathetic dialogue","metadata":{}},{"cell_type":"markdown","source":"!git clone https://github.com/thu-coai/Emotional-Support-Conversation.git","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load dataset\ndataset_name = \"Amod/mental_health_counseling_conversations\"\nmental_health_dataset = load_dataset(dataset_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T16:58:52.435361Z","iopub.execute_input":"2025-04-03T16:58:52.435630Z","iopub.status.idle":"2025-04-03T16:58:58.647868Z","shell.execute_reply.started":"2025-04-03T16:58:52.435599Z","shell.execute_reply":"2025-04-03T16:58:58.647070Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.82k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aa23572e219400d9809ff8f36fb5c83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"combined_dataset.json:   0%|          | 0.00/4.79M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60ca9d59a74c4cfbb18102098bd25206"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3512 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d08bc90741e4470911dfd971ea04077"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"mental_health_dataset[\"train\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:11:06.340136Z","iopub.execute_input":"2025-03-27T12:11:06.340410Z","iopub.status.idle":"2025-03-27T12:11:06.345743Z","shell.execute_reply.started":"2025-03-27T12:11:06.340386Z","shell.execute_reply":"2025-03-27T12:11:06.344871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the mental health dataset\ntrain_test_split = mental_health_dataset[\"train\"].train_test_split(test_size=0.2)\ntrain_dataset = train_test_split[\"train\"]\ntest_dataset = train_test_split[\"test\"]\n\n\nprint(\"Dataset formatted for QLoRA fine-tuning.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:02:42.934219Z","iopub.execute_input":"2025-04-03T17:02:42.934511Z","iopub.status.idle":"2025-04-03T17:02:42.951570Z","shell.execute_reply.started":"2025-04-03T17:02:42.934489Z","shell.execute_reply":"2025-04-03T17:02:42.950834Z"}},"outputs":[{"name":"stdout","text":"Dataset formatted for QLoRA fine-tuning.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"len(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:11:06.374070Z","iopub.execute_input":"2025-03-27T12:11:06.374348Z","iopub.status.idle":"2025-03-27T12:11:06.383291Z","shell.execute_reply.started":"2025-03-27T12:11:06.374327Z","shell.execute_reply":"2025-03-27T12:11:06.382687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# model_path = '/kaggle/input/phi2_train_on_1_epoch_3000/other/default/2/fine_tuned_phi2_multiconv'\nmodel_name = \"microsoft/Phi-3-mini-4k-instruct\"\n\n\n# Configure 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True\n)\n\n# Load model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float16 \n)\n\nprint(\"✅ Phi3.5 LLM Loaded with LoRA and 4-bit Precision!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:11:06.384067Z","iopub.execute_input":"2025-03-27T12:11:06.384347Z","iopub.status.idle":"2025-03-27T12:12:02.035194Z","shell.execute_reply.started":"2025-03-27T12:11:06.384305Z","shell.execute_reply":"2025-03-27T12:12:02.034523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n# Apply LoRA for memory-efficient fine-tuning\nlora_config = LoraConfig(\n    r=4,  # Low-rank adaptation size\n    lora_alpha=32,\n    target_modules=[\"qkv_proj\"],  # Apply LoRA to attention layers\n    lora_dropout=0.05,\n    bias=\"none\"\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:12:02.035906Z","iopub.execute_input":"2025-03-27T12:12:02.036524Z","iopub.status.idle":"2025-03-27T12:12:02.506753Z","shell.execute_reply.started":"2025-03-27T12:12:02.036499Z","shell.execute_reply":"2025-03-27T12:12:02.505971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"EOS Token:\", tokenizer.eos_token)  # Should print \"<|endoftext|>\"\nprint(\"EOS Token ID:\", tokenizer.eos_token_id)  # Should print an integer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:12:02.507574Z","iopub.execute_input":"2025-03-27T12:12:02.507909Z","iopub.status.idle":"2025-03-27T12:12:07.986619Z","shell.execute_reply.started":"2025-03-27T12:12:02.507874Z","shell.execute_reply":"2025-03-27T12:12:07.985544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_length = model.config.max_position_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:03:45.527420Z","iopub.execute_input":"2025-04-03T17:03:45.527770Z","iopub.status.idle":"2025-04-03T17:03:45.531360Z","shell.execute_reply.started":"2025-04-03T17:03:45.527742Z","shell.execute_reply":"2025-04-03T17:03:45.530614Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import json\nfrom datasets import Dataset\n\ndef format_for_finetuning(entry, tokenizer, max_length):\n    context = entry[\"Context\"]\n    response = entry[\"Response\"]\n\n    # Combine system, user context, and the response.\n    system_message = \"<|system|>\\nYou are a friendly therapist, respond to the users' problems in a helpful manner.<|end|>\\n\"\n    user_message = f\"<|user|>\\n{context}<|end|>\\n\"\n    response_message = f\"<|assistant|>\\n{response}<|end|>\\n<|endoftext|>\"\n\n    # Combine all parts\n    combined_input = system_message + user_message + response_message\n\n    # Tokenize the combined sequence\n    tokenized_input = tokenizer(combined_input, truncation=True, max_length=max_length, padding=\"max_length\")\n\n    # Tokenize each part to get correct indices\n    system_tokens = tokenizer.encode(system_message)\n    user_tokens = tokenizer.encode(user_message)\n    response_tokens = tokenizer.encode(response_message)\n\n    # Create labels: -100 for the system and user parts, and valid labels for the response\n    labels = [-100] * len(system_tokens) + [-100] * len(user_tokens) + response_tokens\n    # Truncate labels to max_length to avoid exceeding the model's limit\n    labels = labels[:max_length]\n    \n    padding_length = len(tokenized_input[\"input_ids\"]) - len(labels)\n    if padding_length > 0:\n        # Pad labels on the left (to match padding of the tokenized input)\n        labels = [-100] * padding_length + labels\n\n    tokenized_input[\"labels\"] = labels\n\n    return tokenized_input\n\ndef process_and_format(dataset, tokenizer, max_length):\n    # Use map to process data in a memory-efficient way\n    return dataset.map(lambda entry: format_for_finetuning(entry, tokenizer, max_length), batched=False)\n\n# Example usage\ndataset_train_formatted = process_and_format(train_dataset, tokenizer, max_length=max_length)\ndataset_test_formatted = process_and_format(test_dataset, tokenizer, max_length=max_length)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:12:08.740876Z","iopub.execute_input":"2025-03-27T12:12:08.741224Z","iopub.status.idle":"2025-03-27T12:16:06.835115Z","shell.execute_reply.started":"2025-03-27T12:12:08.741185Z","shell.execute_reply":"2025-03-27T12:16:06.834263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(dataset_train_formatted[0])\nprint(len(dataset_train_formatted[0][\"input_ids\"]))\nprint(len(dataset_train_formatted[0][\"attention_mask\"]))\nprint(len(dataset_train_formatted[0][\"labels\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T12:16:06.835949Z","iopub.execute_input":"2025-03-27T12:16:06.836229Z","iopub.status.idle":"2025-03-27T12:16:07.553406Z","shell.execute_reply.started":"2025-03-27T12:16:06.836195Z","shell.execute_reply":"2025-03-27T12:16:07.552519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load model","metadata":{}},{"cell_type":"code","source":"ls -lh /kaggle/working","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nfrom transformers import TrainingArguments\n\nprint(\"✅ WandB Disabled!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    # evaluation_strategy=\"epoch\",\n    evaluation_strategy=\"no\",\n    learning_rate=3e-4,  # Lower learning rate for LoRA fine-tuning\n    per_device_train_batch_size=1,  # Reduce batch size for memory efficiency\n    gradient_accumulation_steps=4,  # Simulate larger batch size\n    num_train_epochs=1,\n    weight_decay=0.01,\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    fp16=True,  # Mixed precision training\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset_train_formatted,\n    # eval_dataset=dataset_test_formatted,\n)\nprint(\"🚀 Trainer Initialized!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.save_model(\"./fine_tuned_phi35_mini\")\ntokenizer.save_pretrained(\"./fine_tuned_phi35_mini\")\nprint(\"✅ Fine-Tuned Model Saved Successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.move(\"./fine_tuned_phi35_mini\", \"/kaggle/working/fine_tuned_phi35_mini\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-26T18:01:58.808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-26T18:01:58.808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# model_path = '/kaggle/input/microsoft-phi-2-trained/pytorch/default/1/fine_tuned_phi2_train2'\nmodel_path = \"/kaggle/input/phi3_train/pytorch/default/1/results (5)/fine_tuned_phi35_mini\"\n\n# Load model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n\nprint(\"✅ Fine-Tuned Model Loaded Successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:00:54.987363Z","iopub.execute_input":"2025-04-03T17:00:54.987700Z","iopub.status.idle":"2025-04-03T17:02:10.757064Z","shell.execute_reply.started":"2025-04-03T17:00:54.987667Z","shell.execute_reply":"2025-04-03T17:02:10.755980Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7af6198e18a44dafb9bd293c8fde8e8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0442ac2569e40319d1cc3d99ef8c20b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b30b05c64344881b4563d6cf08cb563"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8c2bf16f38b46a7a27eac2c150bd9b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76162a34232c49948fa19047bedbf95d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bedb243f01414018b1433c7c3f0d56c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd88fe0930aa46fa9894a099cf419528"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b715dd08d76467bb09f121f3f78fb02"}},"metadata":{}},{"name":"stdout","text":"✅ Fine-Tuned Model Loaded Successfully!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import json\nfrom datasets import Dataset\n\ndef format_conversation_test(data, tokenizer, max_length=1221):\n    \n    context = data[\"Context\"]\n    response = data[\"Response\"]\n    \n    # Combine system, user context, and the response.\n    system_message = \"<|system|>\\nYou are a friendly therapist, respond to the users' problems in a helpful manner.<|end|>\\n\"\n    user_message = f\"<|user|>\\n{context}<|end|>\\n\"\n    response_message = \"<|assistant|>\\n\"\n\n    # Combine all parts for tokenization\n    combined_input = system_message + user_message + response_message\n\n    # Tokenize the combined sequence for test data\n    tokenized_input = tokenizer(combined_input, truncation=True, max_length=max_length)\n    \n    # The test data typically won't have labels, so no need to add them\n    tokenized_input[\"labels\"] = None\n    \n    return tokenized_input\n\ndef process_and_format(dataset, tokenizer, max_length):\n    # Use map to process data in a memory-efficient way\n    return dataset.map(lambda entry: format_conversation_test(entry, tokenizer, max_length), batched=False)\n\ndataset_test_formatted = process_and_format(test_dataset, tokenizer, max_length=max_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:24:15.095807Z","iopub.execute_input":"2025-04-03T17:24:15.096158Z","iopub.status.idle":"2025-04-03T17:24:15.465472Z","shell.execute_reply.started":"2025-04-03T17:24:15.096128Z","shell.execute_reply":"2025-04-03T17:24:15.464546Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/703 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c2241af9ef14ebe980de17274243ed6"}},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"import torch\n\nidx = [1,3,5]\nfor i in range(len(idx)):\n    input_ids = torch.tensor([dataset_test_formatted[i][\"input_ids\"]]).to(model.device)\n\n    # Optional: also pass attention_mask if needed\n    attention_mask = torch.tensor([dataset_test_formatted[i][\"attention_mask\"]]).to(model.device)\n\n    output = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n    )\n    print(tokenizer.decode(output[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:24:18.719830Z","iopub.execute_input":"2025-04-03T17:24:18.720188Z","iopub.status.idle":"2025-04-03T17:24:24.916339Z","shell.execute_reply.started":"2025-04-03T17:24:18.720160Z","shell.execute_reply":"2025-04-03T17:24:24.915469Z"}},"outputs":[{"name":"stdout","text":"You are a friendly therapist, respond to the users' problems in a helpful manner. How does a counselor decide when to end counseling sessions or to terminate working with a client? Counseling is a process, not a product.  The client and the counselor are\nYou are a friendly therapist, respond to the users' problems in a helpful manner. My boyfriend and I have been arguing every night about the same thing. He also tells me that if I go visit my mother out of state he will be gone when I get back. He and my mother do not get along. What should I do? It sounds like you are feeling very frustrated and upset about your relationship. It is important\nYou are a friendly therapist, respond to the users' problems in a helpful manner. I have high functioning autism and I have been on a lot of dating sites like meet me, match, and zoosk. I haven't had any luck on any of the dating sites I have been on.\n\n I really want a boyfriend but I don't know what I should do. I just want to be in a relationship. How can I meet someone? I'm sorry to hear that you've been having a hard time finding a boyfriend.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"Original model","metadata":{}},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_path = \"/kaggle/working/fine_tuned_phi35_mini\"\n# Load model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n\nprint(\"✅ Fine-Tuned Model Loaded Successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_test_formatted = dataset_test_formatted.remove_columns([\"Context\", \"Response\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:19:24.839394Z","iopub.execute_input":"2025-04-03T17:19:24.839704Z","iopub.status.idle":"2025-04-03T17:19:24.845037Z","shell.execute_reply.started":"2025-04-03T17:19:24.839675Z","shell.execute_reply":"2025-04-03T17:19:24.844310Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"idx = [1,3,5]\nfor i in range(len(idx)):\n    input_ids = torch.tensor([dataset_test_formatted[i][\"input_ids\"]]).to(model.device)\n\n    # Optional: also pass attention_mask if needed\n    attention_mask = torch.tensor([dataset_test_formatted[i][\"attention_mask\"]]).to(model.device)\n\n    output = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n    )\n    print(tokenizer.decode(output[0], skip_special_tokens=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}